---
title: "yl2883"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

The following package \textit{yl2883} implements 3 functions necessary for completing Homework 1 from BTRY 6520. You can install this package via


```{r eval=FALSE}
library("devtools")
devtools::install_github('yl2883/yixiaopackage')
```

Now you can import the package:

```{r}
library(yl2883)
```

# Iterative methods for Linear Systems

The first function of the yl2883 package solves $Ax = b$.  Even though $x$ can be obtained by $A^{-1} b$ theoretically, $A^{-1}$ is computationally expensive. Alternatively, we can use iterative approaches.  This package implements 3 iterative approaches: Gauss-Seidel, Jacobi and Parallel Jacobi

Given $A = D + L + U$, where $D$ is the diagonal of $A$ and $L$ and $U$ are the lower and upper triangular part of $A$ respectively, 

Gauss-Seidel iterations can be expressed as $$x^{(k+1)} = (L + D)^{-1} (b - Ux^{(k)})$$.  

Jacobi iterations can be expressed as $$x^{(k+1)} = D^{-1} (b - (L + U) x^{(k)})$$ 

Jacobi can also be solved by parallel computing.

The function `solve_ols` can be used to solve systems $Ax = b$ using either Gauss-Seidel, Jacobi or Parallel.

```{r}
makeA <- function(n, alpha){
  A <- diag(alpha, n)
  A[abs(row(A) - col(A)) == 1] =-1
  return(A)
}

A =makeA(100,2)

###Create the nessary vector
v = rep(c(1,0),50)
b = A%*%v


###Actual application
x0=rep(0,100)

x = yl2883::solve_ols(A, b,x0)
print(norm(x-v,'2'))
```

If you are interested in the full list of parameters, check `?yl2883::solve_ols`.  

# algorithmic leveraging for linear regression

In a linear regression problem, there might be too many data points. Hence solving OLS estimator $$\hat{\beta} = (X' X)^{-1} X' y$$ is computational intensive. To address this problem, given $n$ data points, we only choose a subsample with replacement of size $r$ with sampling probability $\pi=(\pi_1, \ldots, \pi_n)$, and then we can solve the weighted least squares problem with the corresponding sampling probability. 

For a simpler case, we set $\pi_1 = 1/n$, then the subsample is chosen with uniform probability.  

Alternatively, we construct $\pi_i=\frac{h_{ii}}{\sum_j h_{jj}}$'s based on the leverage scores $h_{ii}$, where $H = X (X' X)^{-1} X'$.  

To compare two methods, we can use the following codes provided by `yl2883::algo_leverage`.
```{r fig.align="center"}
set.seed(65206520)
n = 500
X = rt(500,6)
epsilon = rnorm(500, 0, 1)
Y = -X + epsilon

beta.u=yl2883::algo_leverage( Y, X, 10,1,method ="uniform")
beta.w=yl2883::algo_leverage( Y, X, 10,1,method ="weighted")

plot(X,Y)
lines(X,beta.u*X,col='red')
lines(X,beta.w*X,col='green')
legend('topright',c('Uniform','weighted'),
       lwd=2,lty=1:3,col=2:3)
```

We can see that the green line generated from a leverage-based weighted subsample, is more accurate.

If you are interested in the full list of parameters, check `?yl2883::algo_leverage`.

# coordinate descent Elastic Net

The last functional in the yl2883 package fits elastic net to data using coordinate descent algorithm.. Recall elastic net solves the optimization problem 
$$\min_{\beta \in \mathbb{R}^p} \frac{1}{2n} (y - X\beta)^T (y - X\beta) + \lambda \left[ \frac{1}{2} (1 - \alpha) \beta^T \beta + \alpha ||\beta||_1 \right]$$. Given covariate matrix $X$ and response vector $y$, the function `yl2883::elnet_coord` solves exactly the problem:
```{r}
library(mvtnorm)
p=20
n=100
beta=c(2,0,-2,0,1,0,-1,0,rep(0,12))
Sigma=diag(p)

Sigma[1,2]=0.8
Sigma[2,1]=0.8
Sigma[5,6]=0.8
Sigma[6,5]=0.8

set.seed(65206520)
X=rmvnorm(n, sigma=Sigma)
Y=X%*%beta+rnorm(n)

beta = yl2883::elnet_coord(Y,X, beta, alpha=0.5,lambda=0.1)
beta
```


Here is the helper function to obtain a solution path.
```{r}
SolPath=function(X,Y,alpha,n,p){
  if (alpha==0) {alpha=1e-5}
  lmax=max(abs(t(Y-mean(Y)*(1-mean(Y)))%*%X))/(alpha*n)
  lmin=lmax*1e-4
  log.lamb=seq(log(lmin),log(lmax),len=100)
  ls=sort(exp(log.lamb),decreasing=T)
  betas=matrix(0,p,100)
  for (i in 1:100){
    lambda=ls[i]
    betas[,i]=elnet_coord(Y,X, beta, alpha=0.5,lambda)
  }
  return (list(betas=betas, lambdas=ls))
}
```


We can form path plots for the elastic net model with $\alpha=0.5$ (You should run the helper function given in above chunk first):
```{r fig.align="center"}
res=SolPath(X,Y,alpha=0.5,n,p)
betas=res$betas
norms=colSums(abs(betas))
matplot(norms,t(betas),xlab='l1 norm', ylab='coefficient',type='l')
```



If you are interested in the full list of parameters, check `?yl2883::elnet_coord` for more details  

